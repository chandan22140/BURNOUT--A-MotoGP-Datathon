{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104857,"databundleVersionId":12651513,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.base import clone\nfrom xgboost import XGBRegressor, plot_importance\nfrom sklearn.model_selection import TimeSeriesSplit\nimport joblib\nimport matplotlib.pyplot as plt\nimport os\n\n# Set environment variable to prevent XGBoost device mismatch warning\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n# /kaggle/input/burnout-datathon-ieeecsmuj/test.csv\n# Load data\ntrain = pd.read_csv('/kaggle/input/burnout-datathon-ieeecsmuj/train.csv')\nval = pd.read_csv('/kaggle/input/burnout-datathon-ieeecsmuj/val.csv')\ntest = pd.read_csv('/kaggle/input/burnout-datathon-ieeecsmuj/test.csv')\n\n# Feature engineering with improved safety\ndef create_features(df):\n    # Temporal features\n    df['season_progress'] = df['year_x'] - df['min_year']\n    df['career_phase'] = (df['year_x'] - df['min_year']) / (df['years_active'].replace(0, np.nan) + 1e-6)\n    \n    # Team performance metrics\n    df['team_success_rate'] = df['podiums'] / (df['starts'].replace(0, np.nan) + 1e-6)\n    df['team_reliability'] = df['finishes'] / (df['starts'].replace(0, np.nan) + 1e-6)\n    \n    # Tire dynamics\n    df['tire_wear_ratio'] = df['Tire_Degradation_Factor_per_Lap'] * df['Laps']\n    df['compound_aggressiveness'] = np.where(\n        df['Tire_Compound_Front'] == 'Soft', 1.2,\n        np.where(df['Tire_Compound_Front'] == 'Medium', 1.0, 0.8)\n    )\n    \n    # Track difficulty\n    df['speed_load'] = df['Avg_Speed_kmh'] / (df['Circuit_Length_km'].replace(0, np.nan) + 1e-6)\n    df['corner_intensity'] = df['Corners_per_Lap'] / (df['Circuit_Length_km'].replace(0, np.nan) + 1e-6)\n    \n    # Session progression\n    session_order = {'FP1':1, 'FP2':2, 'FP3':3, 'FP4':4, \n                    'Qualifying':5, 'Sprint':6, 'Race':7}\n    df['session_importance'] = df['Session'].map(session_order).fillna(0)\n    \n    # List of all potential numerical columns\n    numerical_cols = ['Circuit_Length_km', 'Laps', 'Avg_Speed_kmh', \n                     'Tire_Degradation_Factor_per_Lap', 'Ambient_Temperature_Celsius',\n                     'Humidity_%', 'points', 'Championship_Points', 'Corners_per_Lap',\n                     'Pit_Stop_Duration_Seconds', 'Track_Temperature_Celsius', 'air',\n                     'ground', 'starts', 'finishes', 'with_points', 'podiums', 'wins',\n                     'min_year', 'max_year', 'years_active', 'year_x',\n                     'season_progress', 'career_phase', 'team_success_rate', \n                     'team_reliability', 'tire_wear_ratio', 'compound_aggressiveness',\n                     'speed_load', 'corner_intensity', 'session_importance']\n    \n    # Convert to numeric and fill NaN with 0\n    for col in numerical_cols:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n            \n    return df\n\n# Apply feature engineering\ntrain = create_features(train)\nval = create_features(val)\ntest = create_features(test)\n\n# Define feature groups\nhigh_cardinality = ['rider_name', 'circuit_name', 'team_name', 'shortname', 'bike_name']\nordinal_features = ['Tire_Compound_Front', 'Tire_Compound_Rear', \n                   'Grid_Position', 'Championship_Position']\nnominal_features = ['category_x', 'Track_Condition', 'Penalty', \n                   'weather', 'track', 'Session']\n\nnumerical_features = [\n    'Circuit_Length_km', 'Laps', 'Avg_Speed_kmh', 'Tire_Degradation_Factor_per_Lap',\n    'season_progress', 'career_phase', 'team_success_rate', 'team_reliability',\n    'tire_wear_ratio', 'compound_aggressiveness', 'speed_load', 'corner_intensity',\n    'session_importance', 'Ambient_Temperature_Celsius', 'Humidity_%', 'points',\n    'Championship_Points', 'Corners_per_Lap', 'Pit_Stop_Duration_Seconds',\n    'Track_Temperature_Celsius', 'air', 'ground', 'starts', 'finishes', 'with_points',\n    'podiums', 'wins', 'min_year', 'max_year', 'years_active', 'year_x'\n]\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer([\n    ('target_enc', OrdinalEncoder(\n        handle_unknown='use_encoded_value', \n        unknown_value=-1\n    ), high_cardinality),\n    ('ordinal', OrdinalEncoder(\n        categories=[\n            ['Hard','Medium','Soft'], \n            ['Hard','Medium','Soft'],\n            list(range(1, 26)),  # Grid positions\n            list(range(1, 31))   # Championship positions\n        ],\n        handle_unknown='use_encoded_value',\n        unknown_value=-1\n    ), ordinal_features),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'), nominal_features),\n    ('num', 'passthrough', numerical_features)\n])\n\n# Prepare data\nfeatures_used = high_cardinality + ordinal_features + nominal_features + numerical_features\nX_train = train[features_used]\ny_train = train['Lap_Time_Seconds']\nX_val = val[features_used]\ny_val = val['Lap_Time_Seconds']\nX_test = test[features_used]\n\n# Combine train and val for final training\nX_train_val = pd.concat([X_train, X_val])\ny_train_val = pd.concat([y_train, y_val])\n\n# Preprocess all data at once\npreprocessor.fit(X_train_val)\nX_train_val_trans = preprocessor.transform(X_train_val)\nX_test_trans = preprocessor.transform(X_test)\n\n# Create validation split for plotting\nsplit_index = int(len(X_train_val_trans) * 0.9)  # 90% train, 10% validation\nX_final_train = X_train_val_trans[:split_index]\ny_final_train = y_train_val.iloc[:split_index]\nX_final_val = X_train_val_trans[split_index:]\ny_final_val = y_train_val.iloc[split_index:]\n\n# Configure final model\nfinal_model = XGBRegressor(\n    tree_method='hist',\n    device='cuda',\n    n_estimators=20000,\n    learning_rate=0.03770760297401682,\n    max_depth=20,\n    subsample=0.6936774881064425,\n    colsample_bytree=0.851962030829581,\n    gamma=0.146963904565,\n    reg_alpha=1.53108214125,\n    reg_lambda=1.9924755681,\n    eval_metric='rmse',\n    early_stopping_rounds=50\n)\n\n# Train with validation set for learning curve\nhistory = final_model.fit(\n    X_final_train, y_final_train,\n    eval_set=[(X_final_val, y_final_val)],\n    verbose=100  # Print progress every 10 iterations\n)\n\n# Plot learning curve\nresults = final_model.evals_result()\nepochs = len(results['validation_0']['rmse'])\nx_axis = range(0, epochs)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x_axis, results['validation_0']['rmse'], label='Validation')\nplt.legend()\nplt.ylabel('RMSE')\nplt.xlabel('Epochs')\nplt.title('XGBoost Learning Curve')\nplt.savefig('learning_curve.png')\nplt.close()\n\n# Plot feature importance\nplt.figure(figsize=(16, 12))\nplot_importance(final_model, max_num_features=30)\nplt.tight_layout()\nplt.savefig('feature_importance.png')\nplt.close()\n\n# Retrain on full dataset\nfinal_model = XGBRegressor(\n    tree_method='hist',\n    device='cuda',\n    n_estimators=final_model.best_iteration,\n    learning_rate=0.025,\n    max_depth=9,\n    subsample=0.75,\n    colsample_bytree=0.85,\n    gamma=0.1,\n    reg_alpha=0.5,\n    reg_lambda=0.5,\n    eval_metric='rmse'\n)\n\nfinal_model.fit(X_train_val_trans, y_train_val)\n\n# Generate predictions\n# Set device to CPU for prediction to avoid device mismatch\nfinal_model.set_params(device='cpu')\ntest_preds = final_model.predict(X_test_trans)\n\n# Create submission\nsubmission = pd.DataFrame({\n    'Unique ID': test['Unique ID'],\n    'Lap_Time_Seconds': np.round(test_preds, 3)\n})\nsubmission.to_csv('solution.csv', index=False)\n\n# Save model artifacts\njoblib.dump(preprocessor, 'preprocessor.pkl')\njoblib.dump(final_model, 'final_model.pkl')\n\nprint(\"Training completed successfully!\")\nprint(f\"Learning curve saved as 'learning_curve.png'\")\nprint(f\"Feature importance plot saved as 'feature_importance.png'\")\nprint(f\"Submission file saved as 'solution.csv'\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:02:26.860450Z","iopub.execute_input":"2025-06-14T15:02:26.860720Z","iopub.status.idle":"2025-06-14T15:05:33.004178Z","shell.execute_reply.started":"2025-06-14T15:02:26.860701Z","shell.execute_reply":"2025-06-14T15:05:33.003581Z"}},"outputs":[{"name":"stdout","text":"[0]\tvalidation_0-rmse:11.33351\n[100]\tvalidation_0-rmse:1.76940\n[200]\tvalidation_0-rmse:0.37775\n[300]\tvalidation_0-rmse:0.12755\n[400]\tvalidation_0-rmse:0.11570\n[483]\tvalidation_0-rmse:0.11464\nTraining completed successfully!\nLearning curve saved as 'learning_curve.png'\nFeature importance plot saved as 'feature_importance.png'\nSubmission file saved as 'solution.csv'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1600x1200 with 0 Axes>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom xgboost import XGBRegressor, plot_importance\nfrom sklearn.model_selection import TimeSeriesSplit\nimport joblib\nimport matplotlib.pyplot as plt\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load data/kaggle/input/burnout-datathon-ieeecsmuj/test.csv\ntrain = pd.read_csv('/kaggle/input/burnout-datathon-ieeecsmuj/train.csv')\nval = pd.read_csv('/kaggle/input/burnout-datathon-ieeecsmuj/val.csv')\ntest = pd.read_csv('/kaggle/input/burnout-datathon-ieeecsmuj/test.csv')\n\n# Advanced feature engineering\ndef create_features(df):\n    # Temporal features\n    df['years_since_debut'] = df['year_x'] - df['min_year']\n    df['career_phase'] = df['years_since_debut'] / (df['years_active'] + 1e-6)\n    \n    # Performance features\n    df['podium_rate'] = df['podiums'] / (df['starts'] + 1e-6)\n    df['finish_rate'] = df['finishes'] / (df['starts'] + 1e-6)\n    df['win_rate'] = df['wins'] / (df['starts'] + 1e-6)\n    \n    # Tire and track dynamics\n    df['tire_wear_effect'] = df['Tire_Degradation_Factor_per_Lap'] * df['Laps'] * df['Circuit_Length_km']\n    df['compound_aggressiveness'] = np.select(\n        [\n            df['Tire_Compound_Front'] == 'Soft',\n            df['Tire_Compound_Front'] == 'Medium',\n            df['Tire_Compound_Front'] == 'Hard'\n        ],\n        [1.2, 1.0, 0.8],\n        default=1.0\n    )\n    \n    # Speed and corner dynamics\n    df['speed_intensity'] = df['Avg_Speed_kmh'] / df['Circuit_Length_km']\n    df['corner_intensity'] = df['Corners_per_Lap'] / df['Circuit_Length_km']\n    df['speed_corner_ratio'] = df['Avg_Speed_kmh'] / (df['Corners_per_Lap'] + 1e-6)\n    \n    # Session importance\n    session_order = {'FP1':1, 'FP2':2, 'FP3':3, 'FP4':4, \n                    'Qualifying':5, 'Sprint':6, 'Race':7}\n    df['session_importance'] = df['Session'].map(session_order).fillna(0)\n    \n    # Weather effects\n    df['temp_humidity_effect'] = df['Ambient_Temperature_Celsius'] * df['Humidity_%'] / 100\n    df['track_temp_effect'] = df['Track_Temperature_Celsius'] - df['Ambient_Temperature_Celsius']\n    \n    # Rider-team synergy\n    df['rider_team_experience'] = df['year_x'] - df.groupby(['rider_name', 'team_name'])['year_x'].transform('min')\n    # df = df.sort_values('year_x')\n    # group = df.groupby(['rider_name', 'team_name'])\n    # df['rider_team_experience'] = group['year_x'].expanding().apply(lambda x: x.iloc[-1] - x.iloc[0], raw=False)\n\n    # Recent performance\n    df['recent_points'] = df.groupby('rider_name')['points'].transform(lambda x: x.rolling(3, min_periods=1).mean())\n    #=================================================\n    # df['tire_temp_diff'] = df['Track_Temperature_Celsius'] - {\n    #     'Soft': 30, 'Medium': 40, 'Hard': 50\n    # }[df['Tire_Compound_Front']]\n    df['tire_temp_diff'] = df['Track_Temperature_Celsius'] - df['Tire_Compound_Front'].map({\n        'Soft': 30, \n        'Medium': 40, \n        'Hard': 50\n    })\n        \n    df['rain_effect'] = np.where(df['weather'] == 'Raining', \n                                 df['Humidity_%'] * df['Tire_Degradation_Factor_per_Lap'], \n                                 0)\n    #=================================================\n\n    # Numerical columns to process\n    num_cols = [\n        'Circuit_Length_km', 'Laps', 'Avg_Speed_kmh', 'Tire_Degradation_Factor_per_Lap',\n        'Ambient_Temperature_Celsius', 'Humidity_%', 'points', 'Championship_Points',\n        'Corners_per_Lap', 'Pit_Stop_Duration_Seconds', 'Track_Temperature_Celsius',\n        'years_since_debut', 'career_phase', 'podium_rate', 'finish_rate', 'win_rate',\n        'tire_wear_effect', 'compound_aggressiveness', 'speed_intensity', \n        'corner_intensity', 'speed_corner_ratio', 'session_importance',\n        'temp_humidity_effect', 'track_temp_effect', 'rider_team_experience', 'recent_points','tire_temp_diff', 'rain_effect', 'recent_points'\n    ]\n    \n    # Process numerical columns\n    for col in num_cols:\n        if col in df.columns:\n            # Handle infinite values and NaNs\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n            mean_val = df[col].mean()\n            df[col] = df[col].fillna(mean_val if not np.isnan(mean_val) else 0)\n            \n    return df\n\n# Apply feature engineering\ntrain = create_features(train)\nval = create_features(val)\ntest = create_features(test)\n\n# Define feature groups\nhigh_cardinality = ['rider_name', 'circuit_name', 'team_name', 'shortname', 'bike_name']\nordinal_features = ['Tire_Compound_Front', 'Tire_Compound_Rear', \n                   'Grid_Position', 'Championship_Position']\nnominal_features = ['category_x', 'Track_Condition', 'Penalty', \n                   'weather', 'track', 'Session']\n\nnumerical_features = [\n    'Circuit_Length_km', 'Laps', 'Avg_Speed_kmh', 'Tire_Degradation_Factor_per_Lap',\n    'Ambient_Temperature_Celsius', 'Humidity_%', 'points', 'Championship_Points',\n    'Corners_per_Lap', 'Pit_Stop_Duration_Seconds', 'Track_Temperature_Celsius',\n    'years_since_debut', 'career_phase', 'podium_rate', 'finish_rate', 'win_rate',\n    'tire_wear_effect', 'compound_aggressiveness', 'speed_intensity', \n    'corner_intensity', 'speed_corner_ratio', 'session_importance',\n    'temp_humidity_effect', 'track_temp_effect', 'rider_team_experience', 'recent_points'\n]\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer([\n    ('target_enc', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), \n     high_cardinality),\n    ('ordinal', OrdinalEncoder(\n        categories=[\n            ['Hard','Medium','Soft'], \n            ['Hard','Medium','Soft'],\n            list(range(1, 26)),  # Grid positions   \n            list(range(1, 31))   # Championship positions\n        ], \n        handle_unknown='use_encoded_value',\n        unknown_value=-1\n    ), ordinal_features),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), nominal_features),\n    ('num', 'passthrough', numerical_features)\n])\n\n# Prepare data\nfeatures_used = high_cardinality + ordinal_features + nominal_features + numerical_features\nX_train = train[features_used]\ny_train = train['Lap_Time_Seconds']\nX_val = val[features_used]\ny_val = val['Lap_Time_Seconds']\nX_test = test[features_used]\n\n# Combine train and val\nX_full = pd.concat([X_train, X_val])\ny_full = pd.concat([y_train, y_val])\n\n# Preprocess data\npreprocessor.fit(X_full)\nX_train_pre = preprocessor.transform(X_train)\nX_val_pre = preprocessor.transform(X_val)\nX_full_pre = preprocessor.transform(X_full)\nX_test_pre = preprocessor.transform(X_test)\n\n# Hyperparameter optimization with Optuna\ndef objective(trial):\n    params = {\n        'tree_method': 'hist',\n        'device': 'cuda',\n        'n_estimators': 5000,\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n        'max_depth': trial.suggest_int('max_depth',10, 20),\n        'subsample': trial.suggest_float('subsample', 0.4, 0.8),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'gamma': trial.suggest_float('gamma', 0, 1),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0, 5),\n        'eval_metric': 'rmse',\n        'early_stopping_rounds': 100\n    }\n    \n    model = XGBRegressor(**params)\n    \n    model.fit(\n        X_train_pre, y_train,\n        eval_set=[(X_val_pre, y_val)],\n        verbose=False\n    )\n    \n    # Get best score\n    best_rmse = min(model.evals_result()['validation_0']['rmse'])\n    return best_rmse\n\n# Run optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50, timeout=3600)  # 1 hour timeout\n\n# Get best parameters\nbest_params = study.best_params\nprint(f\"Best parameters: {best_params}\")\nprint(f\"Best validation RMSE: {study.best_value:.6f}\")\n\n# Train final model with best parameters\nfinal_model = XGBRegressor(\n    **best_params,\n    tree_method='hist',\n    device='cuda'\n)\n\n# Train with early stopping\nfinal_model.fit(\n    X_train_pre, y_train,\n    eval_set=[(X_val_pre, y_val)],\n    verbose=100\n)\n\n# Plot learning curve\nresults = final_model.evals_result()\nepochs = len(results['validation_0']['rmse'])\nx_axis = range(0, epochs)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x_axis, results['validation_0']['rmse'], label='Validation')\nplt.axhline(y=study.best_value, color='r', linestyle='--', label=f'Best RMSE: {study.best_value:.6f}')\nplt.legend()\nplt.ylabel('RMSE')\nplt.xlabel('Epochs')\nplt.title('XGBoost Learning Curve')\nplt.savefig('learning_curve.png')\nplt.close()\n\n# Plot feature importance\nplt.figure(figsize=(16, 12))\nplot_importance(final_model, max_num_features=30)\nplt.tight_layout()\nplt.savefig('feature_importance.png')\nplt.close()\n\n# Generate predictions\ntest_preds = final_model.predict(X_test_pre)\n\n# Create submission\nsubmission = pd.DataFrame({\n    'Unique ID': test['Unique ID'],\n    'Lap_Time_Seconds': np.round(test_preds, 6)  # More precision\n})\nsubmission.to_csv('solution.csv', index=False)\n\n# # Save model artifacts\n# joblib.dump(preprocessor, 'preprocessor.pkl')\n# joblib.dump(final_model, 'final_model.pkl')\n\n# print(f\"Final model trained with validation RMSE: {study.best_value:.6f}\")\n# print(\"Submission file created with high-precision predictions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:30:57.480326Z","iopub.execute_input":"2025-06-14T15:30:57.481056Z","iopub.status.idle":"2025-06-14T16:16:57.271697Z","shell.execute_reply.started":"2025-06-14T15:30:57.481025Z","shell.execute_reply":"2025-06-14T16:16:57.270462Z"}},"outputs":[{"name":"stderr","text":"[I 2025-06-14 15:31:39,610] A new study created in memory with name: no-name-c5110ed3-c6e3-45de-92e7-ce48a7ffd688\n[I 2025-06-14 15:36:04,657] Trial 0 finished with value: 0.2950977346183734 and parameters: {'learning_rate': 0.03404520225692639, 'max_depth': 12, 'subsample': 0.5378664558053667, 'colsample_bytree': 0.7312894225838537, 'gamma': 0.1592056224129913, 'reg_alpha': 0.18087492390629234, 'reg_lambda': 3.766542462825125}. Best is trial 0 with value: 0.2950977346183734.\n[I 2025-06-14 15:43:20,095] Trial 1 finished with value: 0.1319071050942873 and parameters: {'learning_rate': 0.011402872059158305, 'max_depth': 18, 'subsample': 0.6627781711580402, 'colsample_bytree': 0.876682342023657, 'gamma': 0.10717525341150558, 'reg_alpha': 0.6025606773970926, 'reg_lambda': 2.1170292796479124}. Best is trial 1 with value: 0.1319071050942873.\n[I 2025-06-14 15:48:31,438] Trial 2 finished with value: 0.32998954254633905 and parameters: {'learning_rate': 0.012401106600149989, 'max_depth': 15, 'subsample': 0.6485948034601106, 'colsample_bytree': 0.919865586839708, 'gamma': 0.7024547679833739, 'reg_alpha': 1.2685958882282322, 'reg_lambda': 4.538780647855314}. Best is trial 1 with value: 0.1319071050942873.\n[I 2025-06-14 15:51:52,996] Trial 3 finished with value: 0.21052966334758677 and parameters: {'learning_rate': 0.042733875450584075, 'max_depth': 20, 'subsample': 0.4693604594852423, 'colsample_bytree': 0.7679262955266679, 'gamma': 0.13667994660710348, 'reg_alpha': 1.8561317557855217, 'reg_lambda': 3.7333691928326047}. Best is trial 1 with value: 0.1319071050942873.\n[I 2025-06-14 16:07:07,336] Trial 4 finished with value: 0.2429870536408513 and parameters: {'learning_rate': 0.004343475362912011, 'max_depth': 19, 'subsample': 0.582406281773938, 'colsample_bytree': 0.803072295444424, 'gamma': 0.5822401007926893, 'reg_alpha': 4.591930643747208, 'reg_lambda': 4.441898701734769}. Best is trial 1 with value: 0.1319071050942873.\n[I 2025-06-14 16:13:00,688] Trial 5 finished with value: 0.26235778801330395 and parameters: {'learning_rate': 0.012704498049409195, 'max_depth': 15, 'subsample': 0.6285959800117902, 'colsample_bytree': 0.7896366164435575, 'gamma': 0.39988203058907323, 'reg_alpha': 1.091407286638348, 'reg_lambda': 3.2989943100734607}. Best is trial 1 with value: 0.1319071050942873.\n[I 2025-06-14 16:15:45,925] Trial 6 finished with value: 0.30021429950718526 and parameters: {'learning_rate': 0.023970375102990733, 'max_depth': 18, 'subsample': 0.7315849635814182, 'colsample_bytree': 0.7599688583447313, 'gamma': 0.819461906754533, 'reg_alpha': 2.7731398969798327, 'reg_lambda': 4.974585923227935}. Best is trial 1 with value: 0.1319071050942873.\n[W 2025-06-14 16:16:57,251] Trial 7 failed with parameters: {'learning_rate': 0.019124013902487597, 'max_depth': 20, 'subsample': 0.6535574297575906, 'colsample_bytree': 0.8157062657496101, 'gamma': 0.7143742027743424, 'reg_alpha': 0.23764798555115096, 'reg_lambda': 0.06330159241948852} because of the following error: KeyboardInterrupt().\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/tmp/ipykernel_35/1899386451.py\", line 178, in objective\n    model.fit(\n  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 730, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\", line 1090, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 730, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/xgboost/training.py\", line 181, in train\n    bst.update(dtrain, i, obj)\n  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 2051, in update\n    _LIB.XGBoosterUpdateOneIter(\nKeyboardInterrupt\n[W 2025-06-14 16:16:57,253] Trial 7 failed with value None.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1899386451.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;31m# Run optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 1 hour timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;31m# Get best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1899386451.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mX_train_pre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_pre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1088\u001b[0m                 \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             )\n\u001b[0;32m-> 1090\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1091\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             _check_call(\n\u001b[0;32m-> 2051\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom xgboost import XGBRegressor, plot_importance\nimport joblib\nimport matplotlib.pyplot as plt\nimport os\n\n# Set environment variables\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n# Load data\ntrain = pd.read_csv('/kaggle/input/burnout-datathon-ieeecsmuj/train.csv')\nval = pd.read_csv('/kaggle/input/burnout-datathon-ieeecsmuj/val.csv')\ntest = pd.read_csv('/kaggle/input/burnout-datathon-ieeecsmuj/test.csv')\n\n# Advanced feature engineering\ndef create_features(df):\n    # Temporal features\n    df['years_since_debut'] = df['year_x'] - df['min_year']\n    df['career_phase'] = df['years_since_debut'] / (df['years_active'].replace(0, np.nan) + 1e-6)\n    \n    # Performance features\n    df['podium_rate'] = df['podiums'] / (df['starts'].replace(0, np.nan) + 1e-6)\n    df['finish_rate'] = df['finishes'] / (df['starts'].replace(0, np.nan) + 1e-6)\n    df['win_rate'] = df['wins'] / (df['starts'].replace(0, np.nan) + 1e-6)\n    \n    # Tire and track dynamics\n    df['tire_wear_effect'] = df['Tire_Degradation_Factor_per_Lap'] * df['Laps'] * df['Circuit_Length_km']\n    df['compound_aggressiveness'] = np.select(\n        [\n            df['Tire_Compound_Front'] == 'Soft',\n            df['Tire_Compound_Front'] == 'Medium',\n            df['Tire_Compound_Front'] == 'Hard'\n        ],\n        [1.2, 1.0, 0.8],\n        default=1.0\n    )\n    \n    # Speed and corner dynamics\n    df['speed_intensity'] = df['Avg_Speed_kmh'] / (df['Circuit_Length_km'] + 1e-6)\n    df['corner_intensity'] = df['Corners_per_Lap'] / (df['Circuit_Length_km'] + 1e-6)\n    df['speed_corner_ratio'] = df['Avg_Speed_kmh'] / (df['Corners_per_Lap'] + 1e-6)\n    \n    # Session importance\n    session_order = {'FP1':1, 'FP2':2, 'FP3':3, 'FP4':4, \n                    'Qualifying':5, 'Sprint':6, 'Race':7}\n    df['session_importance'] = df['Session'].map(session_order).fillna(0)\n    \n    # Weather effects\n    df['temp_humidity_effect'] = df['Ambient_Temperature_Celsius'] * df['Humidity_%'] / 100\n    df['track_temp_effect'] = df['Track_Temperature_Celsius'] - df['Ambient_Temperature_Celsius']\n    \n    # Rider-team synergy\n    df['rider_team_experience'] = df.groupby(['rider_name', 'team_name'])['year_x'].transform(lambda x: x - x.min())\n    \n    # Recent performance\n    df['recent_points'] = df.groupby('rider_name')['points'].transform(lambda x: x.rolling(3, min_periods=1).mean())\n    \n    # Tire temperature\n    compound_map = {'Soft': 30, 'Medium': 40, 'Hard': 50}\n    df['tire_temp_diff'] = df['Track_Temperature_Celsius'] - df['Tire_Compound_Front'].map(compound_map)\n    \n    # Rain effect\n    df['rain_effect'] = np.where(df['weather'] == 'Raining', \n                                 df['Humidity_%'] * df['Tire_Degradation_Factor_per_Lap'], \n                                 0)\n    \n    # Numerical columns to process\n    num_cols = [\n        'Circuit_Length_km', 'Laps', 'Avg_Speed_kmh', 'Tire_Degradation_Factor_per_Lap',\n        'Ambient_Temperature_Celsius', 'Humidity_%', 'points', 'Championship_Points',\n        'Corners_per_Lap', 'Pit_Stop_Duration_Seconds', 'Track_Temperature_Celsius',\n        'years_since_debut', 'career_phase', 'podium_rate', 'finish_rate', 'win_rate',\n        'tire_wear_effect', 'compound_aggressiveness', 'speed_intensity', \n        'corner_intensity', 'speed_corner_ratio', 'session_importance',\n        'temp_humidity_effect', 'track_temp_effect', 'rider_team_experience', 'recent_points',\n        'tire_temp_diff', 'rain_effect'\n    ]\n    \n    # Process numerical columns\n    for col in num_cols:\n        if col in df.columns:\n            # Handle infinite values and NaNs\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n            mean_val = df[col].mean()\n            df[col] = df[col].fillna(mean_val if not np.isnan(mean_val) else 0)\n            \n    return df\n\n# Apply feature engineering\ntrain = create_features(train)\nval = create_features(val)\ntest = create_features(test)\n\n# Define feature groups\nhigh_cardinality = ['rider_name', 'circuit_name', 'team_name', 'shortname', 'bike_name']\nordinal_features = ['Tire_Compound_Front', 'Tire_Compound_Rear', \n                   'Grid_Position', 'Championship_Position']\nnominal_features = ['category_x', 'Track_Condition', 'Penalty', \n                   'weather', 'track', 'Session']\n\nnumerical_features = [\n    'Circuit_Length_km', 'Laps', 'Avg_Speed_kmh', 'Tire_Degradation_Factor_per_Lap',\n    'Ambient_Temperature_Celsius', 'Humidity_%', 'points', 'Championship_Points',\n    'Corners_per_Lap', 'Pit_Stop_Duration_Seconds', 'Track_Temperature_Celsius',\n    'years_since_debut', 'career_phase', 'podium_rate', 'finish_rate', 'win_rate',\n    'tire_wear_effect', 'compound_aggressiveness', 'speed_intensity', \n    'corner_intensity', 'speed_corner_ratio', 'session_importance',\n    'temp_humidity_effect', 'track_temp_effect', 'rider_team_experience', 'recent_points',\n    'tire_temp_diff', 'rain_effect'\n]\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer([\n    ('target_enc', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), \n     high_cardinality),\n    ('ordinal', OrdinalEncoder(\n        categories=[\n            ['Hard','Medium','Soft'], \n            ['Hard','Medium','Soft'],\n            list(range(1, 26)),  # Grid positions\n            list(range(1, 31))   # Championship positions\n        ], \n        handle_unknown='use_encoded_value',\n        unknown_value=-1\n    ), ordinal_features),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), nominal_features),\n    ('num', 'passthrough', numerical_features)\n])\n\n# Prepare data\nfeatures_used = high_cardinality + ordinal_features + nominal_features + numerical_features\nX_train = train[features_used]\ny_train = train['Lap_Time_Seconds']\nX_val = val[features_used]\ny_val = val['Lap_Time_Seconds']\nX_test = test[features_used]\n\n# Combine train and val\nX_full = pd.concat([X_train, X_val])\ny_full = pd.concat([y_train, y_val])\n\n# Preprocess data\npreprocessor.fit(X_full)\nX_train_pre = preprocessor.transform(X_train)\nX_val_pre = preprocessor.transform(X_val)\nX_full_pre = preprocessor.transform(X_full)\nX_test_pre = preprocessor.transform(X_test)\n\n# Best hyperparameters from Optuna trial\nbest_params = {\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"tree_method\": \"hist\",\n    \"device\": \"cuda\",\n    \"enable_categorical\": False,\n    \"seed\": 42,\n    \"n_estimators\": 10000,\n    \"learning_rate\": 0.011402872059158305,\n    \"max_depth\": 18,\n    \"subsample\": 0.6627781711580402,\n    \"colsample_bytree\": 0.876682342023657,\n    \"gamma\": 0.10717525341150558,\n    \"reg_alpha\": 0.6025606773970926,\n    \"reg_lambda\": 2.1170292796479124,\n    \"early_stopping_rounds\": 100\n}\n\n# Train model with early stopping\nmodel = XGBRegressor(**best_params)\nmodel.fit(\n    X_train_pre, y_train,\n    eval_set=[(X_val_pre, y_val)],\n    verbose=100\n)\n\n# Get best iteration and RMSE\nbest_iteration = model.best_iteration\nbest_rmse = model.best_score\nprint(f\"Best iteration: {best_iteration}\")\nprint(f\"Best validation RMSE: {best_rmse}\")\n\n# Retrain on full dataset with optimal iterations\nfinal_model = XGBRegressor(\n    **{k: v for k, v in best_params.items() if k not in ['early_stopping_rounds']},\n    n_estimators=best_iteration\n)\nfinal_model.fit(X_full_pre, y_full)\n\n# Plot learning curve\nresults = model.evals_result()\nepochs = len(results['validation_0']['rmse'])\nx_axis = range(0, epochs)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x_axis, results['validation_0']['rmse'], label='Validation')\nplt.axhline(y=best_rmse, color='r', linestyle='--', label=f'Best RMSE: {best_rmse:.6f}')\nplt.legend()\nplt.ylabel('RMSE')\nplt.xlabel('Epochs')\nplt.title('XGBoost Learning Curve')\nplt.savefig('learning_curve.png')\nplt.close()\n\n# Plot feature importance\nplt.figure(figsize=(16, 12))\nplot_importance(model, max_num_features=30)\nplt.tight_layout()\nplt.savefig('feature_importance.png')\nplt.close()\n\n# Generate predictions\ntest_preds = final_model.predict(X_test_pre)\n\n# Create submission\nsubmission = pd.DataFrame({\n    'Unique ID': test['Unique ID'],\n    'Lap_Time_Seconds': np.round(test_preds, 6)\n})\nsubmission.to_csv('solution.csv', index=False)\n\nprint(\"Training completed successfully!\")\nprint(f\"Best validation RMSE: {best_rmse:.6f}\")\nprint(f\"Used {best_iteration} iterations for final model\")\nprint(f\"Learning curve saved as 'learning_curve.png'\")\nprint(f\"Feature importance plot saved as 'feature_importance.png'\")\nprint(f\"Submission file saved as 'solution.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom xgboost import XGBRegressor, plot_importance\nimport joblib\nimport matplotlib.pyplot as plt\nimport os\n\n# Set environment variables\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n# Load data\ntrain = pd.read_csv('/kaggle/input/burnout-datathon-ieeecsmuj/train.csv')\nval = pd.read_csv('/kaggle/input/burnout-datathon-ieeecsmuj/val.csv')\ntest = pd.read_csv('/kaggle/input/burnout-datathon-ieeecsmuj/test.csv')\n\n# Advanced feature engineering\ndef create_features(df):\n    # Temporal features\n    df['years_since_debut'] = df['year_x'] - df['min_year']\n    df['career_phase'] = df['years_since_debut'] / (df['years_active'].replace(0, np.nan) + 1e-6)\n    \n    # Performance features\n    df['podium_rate'] = df['podiums'] / (df['starts'].replace(0, np.nan) + 1e-6)\n    df['finish_rate'] = df['finishes'] / (df['starts'].replace(0, np.nan) + 1e-6)\n    df['win_rate'] = df['wins'] / (df['starts'].replace(0, np.nan) + 1e-6)\n    \n    # Tire and track dynamics\n    df['tire_wear_effect'] = df['Tire_Degradation_Factor_per_Lap'] * df['Laps'] * df['Circuit_Length_km']\n    df['compound_aggressiveness'] = np.select(\n        [\n            df['Tire_Compound_Front'] == 'Soft',\n            df['Tire_Compound_Front'] == 'Medium',\n            df['Tire_Compound_Front'] == 'Hard'\n        ],\n        [1.2, 1.0, 0.8],\n        default=1.0\n    )\n    \n    # Speed and corner dynamics\n    df['speed_intensity'] = df['Avg_Speed_kmh'] / (df['Circuit_Length_km'] + 1e-6)\n    df['corner_intensity'] = df['Corners_per_Lap'] / (df['Circuit_Length_km'] + 1e-6)\n    df['speed_corner_ratio'] = df['Avg_Speed_kmh'] / (df['Corners_per_Lap'] + 1e-6)\n    \n    # Session importance\n    session_order = {'FP1':1, 'FP2':2, 'FP3':3, 'FP4':4, \n                    'Qualifying':5, 'Sprint':6, 'Race':7}\n    df['session_importance'] = df['Session'].map(session_order).fillna(0)\n    \n    # Weather effects\n    df['temp_humidity_effect'] = df['Ambient_Temperature_Celsius'] * df['Humidity_%'] / 100\n    df['track_temp_effect'] = df['Track_Temperature_Celsius'] - df['Ambient_Temperature_Celsius']\n    \n    # Rider-team synergy\n    df['rider_team_experience'] = df.groupby(['rider_name', 'team_name'])['year_x'].transform(lambda x: x - x.min())\n    \n    # Recent performance\n    df['recent_points'] = df.groupby('rider_name')['points'].transform(lambda x: x.rolling(3, min_periods=1).mean())\n    \n    # Tire temperature\n    compound_map = {'Soft': 30, 'Medium': 40, 'Hard': 50}\n    df['tire_temp_diff'] = df['Track_Temperature_Celsius'] - df['Tire_Compound_Front'].map(compound_map)\n    \n    # Rain effect\n    df['rain_effect'] = np.where(df['weather'] == 'Raining', \n                                 df['Humidity_%'] * df['Tire_Degradation_Factor_per_Lap'], \n                                 0)\n    \n    # Numerical columns to process\n    num_cols = [\n        'Circuit_Length_km', 'Laps', 'Avg_Speed_kmh', 'Tire_Degradation_Factor_per_Lap',\n        'Ambient_Temperature_Celsius', 'Humidity_%', 'points', 'Championship_Points',\n        'Corners_per_Lap', 'Pit_Stop_Duration_Seconds', 'Track_Temperature_Celsius',\n        'years_since_debut', 'career_phase', 'podium_rate', 'finish_rate', 'win_rate',\n        'tire_wear_effect', 'compound_aggressiveness', 'speed_intensity', \n        'corner_intensity', 'speed_corner_ratio', 'session_importance',\n        'temp_humidity_effect', 'track_temp_effect', 'rider_team_experience', 'recent_points',\n        'tire_temp_diff', 'rain_effect'\n    ]\n    \n    # Process numerical columns\n    for col in num_cols:\n        if col in df.columns:\n            # Handle infinite values and NaNs\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n            mean_val = df[col].mean()\n            df[col] = df[col].fillna(mean_val if not np.isnan(mean_val) else 0)\n            \n    return df\n\n# Apply feature engineering\ntrain = create_features(train)\nval = create_features(val)\ntest = create_features(test)\n\n# Define feature groups\nhigh_cardinality = ['rider_name', 'circuit_name', 'team_name', 'shortname', 'bike_name']\nordinal_features = ['Tire_Compound_Front', 'Tire_Compound_Rear', \n                   'Grid_Position', 'Championship_Position']\nnominal_features = ['category_x', 'Track_Condition', 'Penalty', \n                   'weather', 'track', 'Session']\n\nnumerical_features = [\n    'Circuit_Length_km', 'Laps', 'Avg_Speed_kmh', 'Tire_Degradation_Factor_per_Lap',\n    'Ambient_Temperature_Celsius', 'Humidity_%', 'points', 'Championship_Points',\n    'Corners_per_Lap', 'Pit_Stop_Duration_Seconds', 'Track_Temperature_Celsius',\n    'years_since_debut', 'career_phase', 'podium_rate', 'finish_rate', 'win_rate',\n    'tire_wear_effect', 'compound_aggressiveness', 'speed_intensity', \n    'corner_intensity', 'speed_corner_ratio', 'session_importance',\n    'temp_humidity_effect', 'track_temp_effect', 'rider_team_experience', 'recent_points',\n    'tire_temp_diff', 'rain_effect'\n]\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer([\n    ('target_enc', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), \n     high_cardinality),\n    ('ordinal', OrdinalEncoder(\n        categories=[\n            ['Hard','Medium','Soft'], \n            ['Hard','Medium','Soft'],\n            list(range(1, 26)),  # Grid positions\n            list(range(1, 31))   # Championship positions\n        ], \n        handle_unknown='use_encoded_value',\n        unknown_value=-1\n    ), ordinal_features),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), nominal_features),\n    ('num', 'passthrough', numerical_features)\n])\n\n# Prepare data\nfeatures_used = high_cardinality + ordinal_features + nominal_features + numerical_features\nX_train = train[features_used]\ny_train = train['Lap_Time_Seconds']\nX_val = val[features_used]\ny_val = val['Lap_Time_Seconds']\nX_test = test[features_used]\n\n# Combine train and val\nX_full = pd.concat([X_train, X_val])\ny_full = pd.concat([y_train, y_val])\n\n# Preprocess data\npreprocessor.fit(X_full)\nX_train_pre = preprocessor.transform(X_train)\nX_val_pre = preprocessor.transform(X_val)\nX_full_pre = preprocessor.transform(X_full)\nX_test_pre = preprocessor.transform(X_test)\n\n# Best hyperparameters from Optuna trial\nbest_params = {\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"tree_method\": \"hist\",\n    \"device\": \"cuda\",\n    \"enable_categorical\": False,\n    \"seed\": 42,\n    \"n_estimators\": 10000,\n    \"learning_rate\": 0.011402872059158305,\n    \"max_depth\": 18,\n    \"subsample\": 0.6627781711580402,\n    \"colsample_bytree\": 0.876682342023657,\n    \"gamma\": 0.10717525341150558,\n    \"reg_alpha\": 0.6025606773970926,\n    \"reg_lambda\": 2.1170292796479124,\n    \"early_stopping_rounds\": 100\n}\n\n# Train model with early stopping\nmodel = XGBRegressor(**best_params)\nmodel.fit(\n    X_train_pre, y_train,\n    eval_set=[(X_val_pre, y_val)],\n    verbose=100\n)\n\n# Get best iteration and RMSE\nbest_iteration = model.best_iteration\nbest_rmse = model.best_score\nprint(f\"Best iteration: {best_iteration}\")\nprint(f\"Best validation RMSE: {best_rmse}\")\n\n# Retrain on full dataset with optimal iterations\nfinal_model = XGBRegressor(\n    **{k: v for k, v in best_params.items() if k not in ['early_stopping_rounds']},\n    n_estimators=best_iteration\n)\nfinal_model.fit(X_full_pre, y_full)\n\n# Plot learning curve\nresults = model.evals_result()\nepochs = len(results['validation_0']['rmse'])\nx_axis = range(0, epochs)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x_axis, results['validation_0']['rmse'], label='Validation')\nplt.axhline(y=best_rmse, color='r', linestyle='--', label=f'Best RMSE: {best_rmse:.6f}')\nplt.legend()\nplt.ylabel('RMSE')\nplt.xlabel('Epochs')\nplt.title('XGBoost Learning Curve')\nplt.savefig('learning_curve.png')\nplt.close()\n\n# Plot feature importance\nplt.figure(figsize=(16, 12))\nplot_importance(model, max_num_features=30)\nplt.tight_layout()\nplt.savefig('feature_importance.png')\nplt.close()\n\n# Generate predictions\ntest_preds = final_model.predict(X_test_pre)\n\n# Create submission\nsubmission = pd.DataFrame({\n    'Unique ID': test['Unique ID'],\n    'Lap_Time_Seconds': np.round(test_preds, 6)\n})\nsubmission.to_csv('solution.csv', index=False)\n\nprint(\"Training completed successfully!\")\nprint(f\"Best validation RMSE: {best_rmse:.6f}\")\nprint(f\"Used {best_iteration} iterations for final model\")\nprint(f\"Learning curve saved as 'learning_curve.png'\")\nprint(f\"Feature importance plot saved as 'feature_importance.png'\")\nprint(f\"Submission file saved as 'solution.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T16:17:42.865995Z","iopub.execute_input":"2025-06-14T16:17:42.866251Z","iopub.status.idle":"2025-06-14T16:26:55.825213Z","shell.execute_reply.started":"2025-06-14T16:17:42.866229Z","shell.execute_reply":"2025-06-14T16:26:55.824288Z"}},"outputs":[{"name":"stdout","text":"[0]\tvalidation_0-rmse:11.46130\n[100]\tvalidation_0-rmse:7.53693\n[200]\tvalidation_0-rmse:5.04374\n[300]\tvalidation_0-rmse:3.54819\n[400]\tvalidation_0-rmse:2.48362\n[500]\tvalidation_0-rmse:1.74803\n[600]\tvalidation_0-rmse:1.22379\n[700]\tvalidation_0-rmse:0.87520\n[800]\tvalidation_0-rmse:0.61089\n[900]\tvalidation_0-rmse:0.43354\n[1000]\tvalidation_0-rmse:0.32635\n[1100]\tvalidation_0-rmse:0.24995\n[1200]\tvalidation_0-rmse:0.20066\n[1300]\tvalidation_0-rmse:0.16915\n[1400]\tvalidation_0-rmse:0.15539\n[1500]\tvalidation_0-rmse:0.15099\n[1600]\tvalidation_0-rmse:0.14798\n[1700]\tvalidation_0-rmse:0.14728\n[1800]\tvalidation_0-rmse:0.14600\n[1900]\tvalidation_0-rmse:0.14572\n[2000]\tvalidation_0-rmse:0.14530\n[2100]\tvalidation_0-rmse:0.14505\n[2200]\tvalidation_0-rmse:0.14463\n[2300]\tvalidation_0-rmse:0.14443\n[2400]\tvalidation_0-rmse:0.14433\n[2500]\tvalidation_0-rmse:0.14412\n[2600]\tvalidation_0-rmse:0.14394\n[2700]\tvalidation_0-rmse:0.14375\n[2800]\tvalidation_0-rmse:0.14374\n[2900]\tvalidation_0-rmse:0.14367\n[3000]\tvalidation_0-rmse:0.14318\n[3100]\tvalidation_0-rmse:0.14296\n[3200]\tvalidation_0-rmse:0.14296\n[3300]\tvalidation_0-rmse:0.14295\n[3400]\tvalidation_0-rmse:0.14293\n[3500]\tvalidation_0-rmse:0.14273\n[3600]\tvalidation_0-rmse:0.14271\n[3700]\tvalidation_0-rmse:0.14229\n[3800]\tvalidation_0-rmse:0.14218\n[3900]\tvalidation_0-rmse:0.14201\n[3938]\tvalidation_0-rmse:0.14201\nBest iteration: 3839\nBest validation RMSE: 0.14200874862238935\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2985111801.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;31m# Retrain on full dataset with optimal iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m final_model = XGBRegressor(\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'early_stopping_rounds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: xgboost.sklearn.XGBRegressor() got multiple values for keyword argument 'n_estimators'"],"ename":"TypeError","evalue":"xgboost.sklearn.XGBRegressor() got multiple values for keyword argument 'n_estimators'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot learning curve\nresults = model.evals_result()\nepochs = len(results['validation_0']['rmse'])\nx_axis = range(0, epochs)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x_axis, results['validation_0']['rmse'], label='Validation')\nplt.axhline(y=best_rmse, color='r', linestyle='--', label=f'Best RMSE: {best_rmse:.6f}')\nplt.legend()\nplt.ylabel('RMSE')\nplt.xlabel('Epochs')\nplt.title('XGBoost Learning Curve')\nplt.savefig('learning_curve.png')\nplt.close()\n\n# Plot feature importance\nplt.figure(figsize=(16, 12))\nplot_importance(model, max_num_features=30)\nplt.tight_layout()\nplt.savefig('feature_importance.png')\nplt.close()\n\n# Generate predictions\ntest_preds = model.predict(X_test_pre)\n\n# Create submission\nsubmission = pd.DataFrame({\n    'Unique ID': test['Unique ID'],\n    'Lap_Time_Seconds': np.round(test_preds, 6)\n})\nsubmission.to_csv('solution.csv', index=False)\n\nprint(\"Training completed successfully!\")\nprint(f\"Best validation RMSE: {best_rmse:.6f}\")\nprint(f\"Used {best_iteration} iterations for final model\")\nprint(f\"Learning curve saved as 'learning_curve.png'\")\nprint(f\"Feature importance plot saved as 'feature_importance.png'\")\nprint(f\"Submission file saved as 'solution.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T16:28:29.490246Z","iopub.execute_input":"2025-06-14T16:28:29.490923Z","iopub.status.idle":"2025-06-14T16:28:33.135360Z","shell.execute_reply.started":"2025-06-14T16:28:29.490901Z","shell.execute_reply":"2025-06-14T16:28:33.134529Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [16:28:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Training completed successfully!\nBest validation RMSE: 0.142009\nUsed 3839 iterations for final model\nLearning curve saved as 'learning_curve.png'\nFeature importance plot saved as 'feature_importance.png'\nSubmission file saved as 'solution.csv'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1600x1200 with 0 Axes>"},"metadata":{}}],"execution_count":3}]}